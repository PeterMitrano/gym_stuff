{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on Policy-In-Model idea thing\n",
    "\n",
    "One nerual network represents the policy. It takes the current state and produces an action. There is one input per state dimension (position, velocity, etc...), and one output per action dimension (move left, move right). For mountain car the output space is discrete, so we one_hot encode the output action. The other neural networks learns a model of the environment. They are trained in succession like GANs, except without network-on-network violence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's what the model looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D(s_{t}, s'_{t+1}) = \\sum_{i=0}^{k}{(s'^{i}_{t+1} - s^{i}_{t})^{2}}$\n",
    "\n",
    "$L(s_{t+1}, s'^{}_{t+1}) = \\sum_{i=0}^{k}{(s'^{i}_{t+1} - s^{i}_{t+1})^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "This model is meant to work best when the reward signal is very sparse. The distance function $D$ is a measure of how well the state is explored. I suspect just euler distance from the last point won't actually work. Maybe some probability density function?\n",
    "\n",
    "It also makes sense to train the environment model more than the policy model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
